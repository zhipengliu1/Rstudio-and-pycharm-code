#Import the libraries for training model
import pandas as pd#Import pandas library for reading csv file.
from sklearn.model_selection import train_test_split#Used to segment data.
import numpy as np#Import numpy data packets to store and process matrices.
import warnings
warnings.filterwarnings("ignore")
import itertools
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor

dataset = pd.read_csv(r'F:\PythonLZP\database_whole_remove\train.csv',encoding='UTF-8')#Import data
print(dataset)
n = dataset.shape[1]
n = n-1

# data:The dataset that needs to be segmented.
# randomstate:Set a random seed to ensure that the same random number is generated every run.
# testsize:The proportion of test set in the whole data set.

testsize=np.array([0.4,0.3,0.2,0.1])#Proportion division of test set
randomstate=np.linspace(1, 10000, 100, dtype=int)#Randomly select 100 integers from 1 to 10000.

#Find the Cartesian product and get the complete permutation and combination.
list_test = list(itertools.product(testsize,randomstate))
columns=['testsize','randomstate']
dataframe_test=pd.DataFrame(list_test,columns=columns)
dataframe_test.insert(dataframe_test.shape[1], 'R2', 0)
#print(dataframe_test)

#Cyclic assignment instruction
cycle=0
for i in dataframe_test['testsize']:
    for j in dataframe_test['randomstate']:
              train_set, test_set = train_test_split(dataset.values, test_size=i,
                                               random_state=j)

        x_train = train_set[:, 0:n]
        y_train = train_set[:, n]
        x_test = test_set[:, 0:n]
        y_test = test_set[:, n]

#gridsearch optimization

# gamma:The minimum loss required for further partition on the leaf nodes of the tree decreases, and the greater the gamma
# subsample:Sub-sample ratio of training instances. Setting it to 0.5 means XGBoost will randomly sample half of the training data before planting trees.
# colsample_bytree:Parameters used for sub-sampling of columns, which are used to control the proportion of columns randomly sampled by each tree.
# alpha:L1 regularization term about weight. Increasing this value will make the model more conservative.
# lambda:L2 regularization term of weight. Increasing this value will make the model more conservative.

cv_params = {'n_estimators': np.linspace(1000, 6000, 20, dtype=int)}
cv_params = {'min_samples_split': np.linspace(1, 100, 100, dtype=int)}
cv_params = {'min_samples_leaf': np.linspace(1, 100, 20, dtype=int)}
cv_params = {'max_depth': np.linspace(1, 10, 10, dtype=int)}
cv_params = {'alpha': np.linspace(0.01, 1, 20, dtype=float)}
cv_params = {'subsample': np.linspace(0.01, 1, 10, dtype=float)}
cv_params = {'max_features': np.linspace(0.001, 1, 10, dtype=float)}
cv_params = {'learning_rate': np.linspace(0.01, 1, 10, dtype=float)}
gs = GridSearchCV(gbr, cv_params,scoring = 'r2',#Using r2 as an evaluation index
 verbose=2,#Output one line of information each time each parameter combination is fitted.
 refit=True,#Re-fitting the model after finding the best parameters.
 cv=10,#Use 10-fold cross validation
 n_jobs=-1)#Use all available CPU cores for calculation.

gs.fit(x_train, y_train)  

print("Optimal value of parameters：:", gs.best_params_)
print("Best model score:", gs.best_score_)


        # Gradient Boosting
        gbr_params = {'n_estimators': 1000,
                      'min_samples_split': 16,
                      'min_samples_leaf': 2,
                      'max_depth': 6,
                      'alpha': 0.68,
                      'subsample': 0.87,
                      'max_features': 1,
                      'learning_rate': 0.1,
                      'random_state': j,
                      'loss': 'huber',
                      'criterion': 'squared_error'}
        gbr = GradientBoostingRegressor(**gbr_params)

        # K-fold cross validation
        n_folds = 10
        kf = KFold(n_folds, shuffle=True, random_state=j).get_n_splits(x_train)
        scores = cross_val_score(gbr, x_train, y_train, scoring='r2',
                                 cv=kf)
        dataframe_test.iloc[cycle, 2] = scores.mean()
        cycle = cycle + 1
        print(cycle)
        # print(scores.mean())

Best_score_row=dataframe_test['R2'].argmax()#Gets the row number of the maximum value of a column.
print(dataframe_test[Best_score_row:Best_score_row+1])
dataframe_test.to_csv('F:\PythonLZP\timeGbr_testsize_randomstate.csv') 

train_set, test_set = train_test_split(dataset.values, test_size=0.1, random_state=3684)

#Model training
from sklearn.metrics import mean_squared_error, r2_score

gbr.fit(x_train,y_train)

y_pred_train = gbr.predict(x_train)
#training set
print(f"RMSE：{np.sqrt(mean_squared_error(y_pred_train, y_train))}")
print(f"training setR^2：{r2_score(y_train, y_pred_train)}")

y_pred_test = gbr.predict(x_test)
#test set
print(f"RMSE：{np.sqrt(mean_squared_error(y_pred_test, y_test))}")
print(f"test setR^2：{r2_score(y_test, y_pred_test)}")

columns=['wateractivity','thickness','diameter','H/D','pressure']
dataframe_test=pd.DataFrame(x_train,columns=columns)

dataframe_test.insert(dataframe_test.shape[1], 'experimental value', (y_train))
dataframe_test.insert(dataframe_test.shape[1], 'predictive value', (y_pred_train))

dataframe_test.to_csv('F:\PythonLZP\timeGbrtrain0116.csv')#Obtain the relative position and save it in the path obtained by getwcd ().


columns=['wateractivity','thickness','diameter','H/D','pressure']
dataframe_test2=pd.DataFrame(x_test,columns=columns)

dataframe_test2.insert(dataframe_test2.shape[1], 'experimental value', (y_test))
dataframe_test2.insert(dataframe_test2.shape[1], 'predictive value', (y_pred_test))

dataframe_test2.to_csv('F:\PythonLZP\timeGbrtest0116.csv') #Obtain the relative position and save it in the path obtained by getwcd ().





import pickle

# save model to file 
pickle.dump(gbr, open("F:\PythonLZP\timeGbr.dat", "wb"))
