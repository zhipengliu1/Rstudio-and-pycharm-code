#Import the libraries for training model
import pandas as pd#Import pandas library for reading csv file
from sklearn.model_selection import train_test_split#Used to segment data.
import numpy as np#Import numpy data packets to store and process matrices.
import itertools
import XGBoost as XGB
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import GridSearchCV

dataset = pd.read_csv(r'E:\PythonLZP\database_wss.csv',encoding='UTF-8')#Import data
print(dataset)
n = dataset.shape[1]#Obtained column
n = n-1

# data:The dataset that needs to be segmented.
# randomstate:Set a random seed to ensure that the same random number is generated every run.
# testsize:The proportion of test set in the whole data set.
testsize=np.array([0.4,0.3,0.2,0.1])#Proportion division of test set
randomstate=np.linspace(1, 10000, 100, dtype=int)##Randomly select 100 integers from 1 to 10000.

#Find the Cartesian product and get the complete permutation and combination.
list_test = list(itertools.product(testsize,randomstate))
columns=['testsize','randomstate']
dataframe_test=pd.DataFrame(list_test,columns=columns)
dataframe_test.insert(dataframe_test.shape[1], 'R2', 0)
#print(dataframe_test)

#Cyclic assignment instruction
for i in dataframe_test['testsize']:
    for j in dataframe_test['randomstate']:
        train_set, test_set = train_test_split(dataset.values, test_size=i,
                                               random_state=j)

        x_train = train_set[:, 0:n]
        y_train = train_set[:, n]
        x_test = test_set[:, 0:n]
        y_test = test_set[:, n]

#gridsearch optimization

# gamma:The minimum loss required for further partition on the leaf nodes of the tree decreases, and the greater the gamma
# subsample:Sub-sample ratio of training instances. Setting it to 0.5 means XGBoost will randomly sample half of the training data before planting trees.
# colsample_bytree:Parameters used for sub-sampling of columns, which are used to control the proportion of columns randomly sampled by each tree.
# alpha:L1 regularization term about weight. Increasing this value will make the model more conservative.
# lambda:L2 regularization term of weight. Increasing this value will make the model more conservative.
cv_params = {'n_estimators': np.linspace(10, 20000, 1000, dtype=int)}
cv_params = {'min_child_weight': np.linspace(1, 100, 100, dtype=int)}
cv_params = {'max_depth': np.linspace(1, 10, 10, dtype=int)}
cv_params = {'gamma': np.linspace(1, 10, 10, dtype=int)}
cv_params = {'subsample': np.linspace(0.05, 1, 100, dtype=float)}
cv_params = {'colsample_bytree': np.linspace(0.05, 10, 100, dtype=float)}
cv_params = {'reg_alpha': np.linspace(0.05, 10, 100, dtype=float)}
cv_params = {'reg_lambda': np.linspace(0.05, 0.2, 100, dtype=float)}
cv_params = {'eta': np.linspace(0.05, 1, 100, dtype=float)}

gs = GridSearchCV(regress_model, cv_params,scoring = 'r2',#Using r2 as an evaluation index
 verbose=2,#Output one line of information each time each parameter combination is fitted.
 refit=True,#Re-fitting the model after finding the best parameters.
 cv=10,#Use 10-fold cross validation
 n_jobs=-1)#Use all available CPU cores for calculation.

gs.fit(x_train, y_train)  

print("Optimal value of parameters：:", gs.best_params_)
print("Best model score:", gs.best_score_)

        # xgboost
        other_params = {'n_estimators': 7273, 'min_child_weight': 12, 'max_depth': 3, 'gamma': 1,
                        'subsample': 0.79, 'colsample_bytree': 0.72, 'reg_alpha': 0.06, 'reg_lambda': 0.18,
                        'eta': 0.3}
        regress_model = xgb.XGBRegressor(**other_params)
         # K-fold cross validation
        n_folds = 10
        kf = KFold(n_folds, shuffle=True, random_state=j).get_n_splits(x_train)
        scores = cross_val_score(regress_model, x_train, y_train, scoring='r2',
                                 cv=kf)
        dataframe_test.iloc[cycle, 2] = scores.mean()
        cycle = cycle + 1

Best_score_row=dataframe_test['R2'].argmax()#Gets the row number of the maximum value of a column.
print(dataframe_test[Best_score_row:Best_score_row+1])
dataframe_test.to_csv('E:\PythonLZP\XGboost_testsize_randomstate.csv') 


train_set, test_set = train_test_split(dataset.values, test_size=0.1, random_state=5455)

#Model training
from sklearn.metrics import mean_squared_error, r2_score
regress_model.fit(x_train,y_train)

y_pred_train = regress_model.predict(x_train)
#training set
print(f"RMSE：{np.sqrt(mean_squared_error(y_pred_train, y_train))}")
print(f"training setR^2：{r2_score(y_train, y_pred_train)}")

y_pred_test = regress_model.predict(x_test)
#test set
print(f"RMSE：{np.sqrt(mean_squared_error(y_pred_test, y_test))}")
print(f"test setR^2：{r2_score(y_test, y_pred_test)}")


#Establish csv files, and compare the predictive value with the experimental value.
columns=['diameter','H/D','pressure']
dataframe_test=pd.DataFrame(x_train,columns=columns)

dataframe_test.insert(dataframe_test.shape[1], 'experimental value', (y_train))
dataframe_test.insert(dataframe_test.shape[1], 'predictive value', (y_pred_train))

dataframe_test.to_csv('E:\PythonLZP\XGboosttrainFull_ML_preidct.csv')#Obtain the relative position and save it in the path obtained by getwcd ().


columns=['diameter','H/D','pressure']
dataframe_test2=pd.DataFrame(x_test,columns=columns)

dataframe_test2.insert(dataframe_test2.shape[1], 'experimental value', (y_test))
dataframe_test2.insert(dataframe_test2.shape[1], 'predictive value', (y_pred_test))

dataframe_test2.to_csv('E:\PythonLZP\XGboosttestFull_ML_preidct.csv') #Obtain the relative position and save it in the path obtained by getwcd ().




predpred = pd.read_csv(r'E:\PythonLZP\database_for_prediction.csv',encoding='UTF-8')
y_predpred = regress_model.predict(predpred)



import pickle

# save model to file
pickle.dump(regress_model, open("E:\PythonLZP\XGboost.dat", "wb"))

